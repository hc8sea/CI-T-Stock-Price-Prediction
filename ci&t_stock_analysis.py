# -*- coding: utf-8 -*-
"""CI&T Stock Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1esT-mMJrK9nX1R361YZTTZJ8ctHdgAhD

O yfinance será utilizado para extrair os dados financeiros da CI&T
"""

#!pip install tensorflow==2.5.0
!pip install yfinance==0.1.67

"""Instale os pacotes necessários"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf
import pandas as pd

def plot_series(time, series, format="-", start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.grid(True)

print(tf.__version__)

"""Extraia as informações financeiras e converta-as em numpy arrays"""

cit = yf.Ticker("CINT")
cit_share_price_data = cit.history(period="max")
cit_share_price_data.head()
cit_share_price_data.reset_index(inplace=True)

series_avg = []
for i,j in zip(cit_share_price_data["High"], cit_share_price_data["Low"]):
  series_avg.append((i+j)/2)

time = []
step = 0
time_open = cit_share_price_data["Date"]
for i in time_open:
  time.append(step)
  step = step + 1
time = np.array(time)

series = []
for i in series_avg:
  i = float(i)
  series.append(i)
series = np.array(series)

plt.figure(figsize=(10, 6))
plot_series(time, series)

"""Esta parte separa os dados de teste e de validação"""

split_time = 41
time_train = time[:split_time]
x_train = series[:split_time]
time_valid = time[split_time:]
x_valid = series[split_time:]

window_size = 20
batch_size = 2
shuffle_buffer_size = 20

"""Defina esta função"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

"""e também esta outra, responsável por prever os valores da ação."""

def model_forecast(model, series, window_size):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size))
    ds = ds.batch(2).prefetch(1)
    forecast = model.predict(ds)
    return forecast

"""Neste ponto, um modelo será treinado apenas para o intuito de encontrar o melhor Learning Rate para o Optimizer"""

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
window_size = 20
batch_size = 2
train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
print(train_set)
print(x_train.shape)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=32, kernel_size=5,
                      strides=1, padding="causal",
                      activation="relu",
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400)
])

lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])

"""O gráfico a seguir mostra qual Learning Rate é este."""

plt.semilogx(history.history["lr"], history.history["loss"])
plt.axis([1e-8, 1e-4, 0, 60])

"""Colocando o melhor valor (que mudará, caso você esteja realizando testes) e efetuando o treinamento definitivo"""

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
#batch_size = 16
dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=32, kernel_size=3,
                      strides=1, padding="causal",
                      activation="relu",
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(32, return_sequences=True),
  tf.keras.layers.LSTM(32, return_sequences=True),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 200)
])

optimizer = tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(dataset,epochs=500)

"""Executando a previsão"""

rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)
rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]

"""**Enfim, o resultado! O gráfico em laranja representa a previsão que a Rede Neural teria feito do preço médio das ações da CI&T nos últimos 7 dias e o gráfico em azul mostra o preço médio que de fato ocorreu no mercado em cada um dos dias.**"""

plt.figure(figsize=(10, 6))
print(time_valid)
plot_series(time_valid, x_valid)
plot_series(time_valid, rnn_forecast)

"""Aqui temos a métrica de erro"""

tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()

"""E aqui a diminuição do Loss e MAE ao longo das 500 iterações"""

import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
mae=history.history['mae']
loss=history.history['loss']

epochs=range(len(loss)) # Get number of epochs

#------------------------------------------------
# Plot MAE and Loss
#------------------------------------------------
plt.plot(epochs, mae, 'r')
plt.plot(epochs, loss, 'b')
plt.title('MAE and Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["MAE", "Loss"])

plt.figure()

epochs_zoom = epochs[200:]
mae_zoom = mae[200:]
loss_zoom = loss[200:]

#------------------------------------------------
# Plot Zoomed MAE and Loss
#------------------------------------------------
plt.plot(epochs_zoom, mae_zoom, 'r')
plt.plot(epochs_zoom, loss_zoom, 'b')
plt.title('MAE and Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["MAE", "Loss"])

plt.figure()